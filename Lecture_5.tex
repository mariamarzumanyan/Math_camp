\documentclass{beamer}

%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
\usetheme{Boadilla}
%\usetheme{boxes}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{default}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}
\usepackage{epsfig}

\usepackage{amssymb, graphicx, amsmath, amsthm}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\setbeamercovered{highly dynamic}
\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}

\title{\textsc{Mathematics for Economics PhD}}


\subtitle{Lecture 5} 
\author{Instructor:  Mariam Arzumanyan}


\date[UIUC, Fall 2021]{University of Illinois at Urbana-Champaign \\August 20, 2021 }


\subject{Lecture Session}

\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}
\pgfdeclareimage[height=0.7cm]{Illinilogo}{Illinilogo.png}
\logo{\pgfuseimage{Illinilogo}} 
% Let's get started
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\setbeamercovered{invisible}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{frame}{Class Information}
\begin{itemize}
  
    \item \textbf{Phone:} **
    \item \textbf{Email:} mariama2@illinois.edu
\item \textbf{Office Hours:}  1:30 - 2:30 pm  by appointment
\item \textbf{Zoom: }**
\item \textbf{During Class:} Please have your Video on and Sound muted. 

\end{itemize}

\end{frame}
\begin{frame}{References}
	\begin{itemize}

\item	\textbf{Simon, Blume, Mathematics for Economists \textit{W.W.Norton, 1994}}
\item	Mas-Colell, Whinston, Green, Microeconomic Theory: Mathematical Appendix \textit{Oxford University Press,  1995 }
\item		Sydsater, Hammond, Seierstad, and Strom, (2008). Further mathematics for economic analysis. Pearson education. Financial Times/Prentice Hall, second edition.
\item		Stokey, Lucas, Prescott, (1989). Recursive Methods in Economics Dynamic, Harvard University Press.

 \end{itemize}   
\end{frame}

\section{Dynamic Programming }
\begin{frame}{Typical Problem }
    \begin{itemize}
        \item Consider the problem of optimal growth (Cass-Koopmans Model). 
        \item Utility is maximized for the representative agent, given the technology they are faced with. 
        \item The objective is to maximize the present discounted value of future utility:
        \[\sum_{t=0}^\infty \beta^t U(c_t).
        \]
        \item Consider the technology:
        \[y_t=f(k_t).
        \]
        \item The law of motion for the capital stock is:
        \[k_{t+1}=k_t(1-\delta)+i_t.
        \]
    \end{itemize}
        
\end{frame}


\begin{frame}{Typical Problem }
\begin{itemize}
    \item We will assume ther is $100\%$ depreciation: $\delta=1$.
    \item Investment and consumption must both come from current production, so the resource constraint for the agent is: 
    \[c_t+i_t=f(k_t), 
    \]
    \item Combing it with the law of motion, we will be using as our budget constraint the following:
    \[c_t+k_{t+1}=f(k_t).
    \]
\end{itemize}
\end{frame}
\begin{frame}{The Social Planner Problem}
    The social planner problem may be written:
    \[\max_{\{c_t, k_{t+1}\}_{t=0}^\infty} \sum_{t=0}^\infty \beta^t U(c_t)
    \]
    such that 
     \[c_t+k_{t+1}=f(k_t),
    \]
    and $k_0$ given.
    \par We will characterize the solution by a function called a policy rule, which tells what the optimal choice is as a function of the current state of the economy. In this case we will find a rule for choosing $c_t$ and $k_{t+1}$ as a function of $k_t$, which applies in each and every period. 
\end{frame}



\section{A Deterministic Finite Horizon Problem}
\begin{frame}{A Deterministic Finite Horizon Problem}
    \begin{itemize}
        \item Assume representative agent/social planner has limited time, with terminal period $T$. 
        \item We will consider using as a solution for the infinite problem the solution we find for the finite horizon problem, when we take a limiting case as $T\to \infty$.
        \item The problem may now be written:
        \[\max_{\{ k_{t+1}\}_{t=0}^T} \sum_{t=0}^\infty \beta^t U\left(f(k_t)-k_{t+1}\right)
    \]
        
    \end{itemize}
\end{frame}
\begin{frame}{First Order Conditions}
    \begin{itemize}
       
        \item The problem is:
        \[\max_{\{ k_{t+1}\}_{t=0}^T} \sum_{t=0}^T \beta^t U\left(f(k_t)-k_{t+1}\right)
    \]
    \item Look at a section of the sum, which pertains to a generic period: 
    \[... +  \beta^t U\left(f(k_t)-k_{t+1}\right)+ \beta^{t+1} U\left(f(k_{t+1})-k_{t+2}\right)+...
    \]
        \item This includes all the appearances for $k_{t+1}$. Take a derivative with respect to $k_{t+1}$:
        \[ - \beta^t U'\left(f(k_t)-k_{t+1}\right)+ \beta^{t+1} U'\left(f(k_{t+1})-k_{t+2}\right)f'(k_{t+1})=0 \text{ for } t<T, 
        \]
        and $k_{T+1}=0$.
    \end{itemize}
\end{frame}
\begin{frame}{First Order Conditions}
    \begin{itemize}
       
        \item The problem is:
        \[\max_{\{ k_{t+1}\}_{t=0}^T} \sum_{t=0}^T \beta^t U\left(f(k_t)-k_{t+1}\right)
    \]
 
        \item  Take a derivative with respect to $k_{t+1}$:
        \[ - \beta^t U'\left(f(k_t)-k_{t+1}\right)+ \beta^{t+1} U'\left(f(k_{t+1})-k_{t+2}\right)f'(k_{t+1})=0 \text{ for } t<T, 
        \]
        rewriting:
        \[  U'\left(f(k_t)-k_{t+1}\right)= \beta U'\left(f(k_{t+1})-k_{t+2}\right)f'(k_{t+1})=0 \text{ for } t<T, 
        \]
         and $k_{T+1}=0$.
    \end{itemize}
\end{frame}
\begin{frame}{A Special Case}
Consider a log utility:
\[U(c_t)=\ln c_t. 
\]
A Cobb-Douglas production function:
\[f(k_t)=k_t^\alpha.
\]
The first order condition becomes:
\[\frac{1}{k_t^\alpha-k_{t+1}}=\beta \left(\frac{1}{k_{t+1}^\alpha-k_{t+2} } \right) \alpha k_{t+1}^{\alpha-1}.
\]
    
\end{frame}
\begin{frame}{A Special Case}
\begin{itemize}
    \item The first order condition becomes:
\[\frac{1}{k_t^\alpha-k_{t+1}}=\beta \left(\frac{1}{k_{t+1}^\alpha-k_{t+2} } \right) \alpha k_{t+1}^{\alpha-1}.
\]
   \item This is a second-order difference equation which is difficult to solve. 
   \item We need to make it into a first-order difference equation by using a change in variable:
   \[z_t=\text{savings rate at time }t=\frac{k_{t+1}}{k_t^\alpha}.
   \]
   \item The first order condition can be written:
   \[\frac{z_t}{1-z_t}=\alpha \beta \frac{1}{1-z_{t+1}}\Rightarrow z_{t+1}=1+\alpha\beta -\frac{\alpha\beta}{z_t}.
   \]
   
\end{itemize}
    
\end{frame}
\begin{frame}{Solving Recursively}

The first order condition can be written:
   \[ z_{t+1}=1+\alpha\beta -\frac{\alpha\beta}{z_t}.
   \]
Start at the boundary point: $z_T=0$. Now solve for $z_{T-1}$:
\[z_{T}=0=1+\alpha\beta -\frac{\alpha\beta}{z_{T-1}}\Rightarrow z_{T-1}=\frac{\alpha\beta}{1+\alpha \beta}.
\]
Now plug this back into the first order condition for the previous period:
\[ z_{T-1}=\frac{\alpha\beta}{1+\alpha \beta}=1+\alpha\beta -\frac{\alpha\beta}{z_{T-2}}
\]
which implies:
\[z_{T-2}=\frac{\alpha\beta(1+\alpha\beta)}{1+\alpha \beta(1+\alpha\beta)}.
\]\end{frame}
\begin{frame}{Solving Recursively}

The first order condition can be written:
   \[ z_{t+1}=1+\alpha\beta -\frac{\alpha\beta}{z_t}.
   \]
   
  If we keep moving backwards:
  \[z_t=\frac{\sum_{s=1}^{T-t}(\alpha\beta)^s}{1+\sum_{s=1}^{T-t}(\alpha\beta)^s}.
  \]
  If we take the limit:
   \[\lim_{T\to \infty}z_t=\lim_{T\to \infty}\frac{\sum_{s=1}^{T-t}(\alpha\beta)^s}{1+\sum_{s=1}^{T-t}(\alpha\beta)^s}=\alpha\beta
  \]
\end{frame}


\section{A Deterministic Infinite Horizon Problem}
\begin{frame}{Infinite Horizon Problem}
    Let's consider the infinite horizon problem:
     \[\max_{\{c_t, k_{t+1}\}_{t=0}^\infty} \sum_{t=0}^\infty \beta^t U(c_t)
    \]
    such that 
     \[c_t+k_{t+1}=f(k_t), \text{ and }k_0 \text{ given.}
    \]
In  general, we can't just find the solution to the infinite horizon problem by taking the limit of the finite-horizon solution as $T\to \infty$.    
    \[\max \lim_{T\to \infty} \sum_{t=0}^T  U(c_t) \neq  \lim_{T\to \infty}\max  \sum_{t=0}^T  U(c_t).
    \]
\end{frame}
\begin{frame}{Value Function}
    Define a function $v(k_0)$, called the \textbf{value function}:
    \[ v(k_0)=\max_{\{c_t, k_{t+1}\}_{t=0}^\infty} \sum_{t=0}^\infty \beta^t U(c_t).
    \]
    Then $v(k_1)$ is the value of utility that can be obtained with a beginning level of capital in period $t=1$ of $k_1$. 
    \[v(k_1)=\max_{\{c_t, k_{t+1}\}_{t=1}^\infty} \sum_{t=1}^\infty \beta^t U(c_t).
    \]
    The same way we can define $v(k_2)$, $v(k_3)$, and so on. 
\end{frame}
\begin{frame}{Recursive Formulation}
    The social planner problem becomes:
    \[ v(k_0)=\max_{\{c_t, k_{t+1}\}_{t=0}^\infty} \sum_{t=0}^\infty \beta^t U(c_t)
    \]
    \[= \max_{c_0, k_1}[U(c_0)+\beta v(k_1)]
    \]
    such that 
    \[c_0+k_1=f(k_0).
    \]
    Rewrite with constraint substituted into objective function:
    \[v(k_0)=\max \left[ U(f(k_0)-k_1)+\beta v(k_1)\right].
    \]
    This is called \textbf{Bellman's equation}.
    \end{frame}
\begin{frame}{Recursive Formulation}
 \[v(k_0)=\max_{k_1} \left[ U(f(k_0)-k_1)+\beta v(k_1)\right].
    \]
 \begin{itemize}
     \item  This is called \textbf{Bellman's equation}. We can regard this as an equation where the argument is the function $v$, a "functional equation".
     \item $k_0$ is called a \textbf{state variable}.  State variables are a complete description of the current position of the system. 
     \item $k_1$ is called a \textbf{control variable}. Control variables are the variables that must be chosen in the current period. 
     \item If consumption $c_0$ had not been substituted out in the equation above, it too would be a control variable. 
 \end{itemize}  
    \end{frame}
\begin{frame}{Recursive Formulation}
 \[v(k_0)=\max_{k_1} \left[ U(f(k_0)-k_1)+\beta v(k_1)\right].
    \]
    The first order conditions for the equation above is:
    \[U'[f(k_0)-k_1]=\beta v'(k_1). 
    \]
    This equates the marginal utility of consuming current output to the marginal utility of allocating it to capital and enjoying augmented consumption next period.
\end{frame}

\begin{frame}{Envelope Theorem}
    We would like to get rid of the term $v'$ in the necessary condition. Assume a solution for the problem exists, and it is just a function of the state variable:
    \[k_1=g(k_0).
    \]
    So \[v(k_0)=\max_{k_1} \left[ U(f(k_0)-k_1)+\beta v(k_1)\right].
    \]
    becomes 
     \[v(k_0)= U(f(k_0)-g(k_0))+\beta v(g(k_0)).
    \]
    \end{frame}

\begin{frame}{Envelope Theorem}
Totally differentiate (everything is a function of $k_0$):
\[v^\prime (k_0)=U^\prime(f(k_0)-g(k_0))[f^\prime(k_0)-g^\prime(k_0)]+\beta v^\prime (g(k_0))g^\prime(k_0).
\]
Rewriting
\[v^\prime (k_0)=U^\prime(f(k_0)-g(k_0))f^\prime(k_0)-\left[U^\prime(f(k_0)-g(k_0))+\beta v^\prime (g(k_0))\right]g^\prime(k_0).
\]
The FOC says the second term equals zero, so
\[v^\prime (k_0)=U^\prime(f(k_0)-g(k_0))f^\prime(k_0).
\]
\[v^\prime (k_0)=U^\prime(f(k_0)-k_1)f^\prime(k_0).
\]
   \end{frame}

\begin{frame}{Envelope Theorem}

\[v^\prime (k_0)=U^\prime(f(k_0)-k_1)f^\prime(k_0).
\]
Update one period 
\[v^\prime (k_1)=U^\prime(f(k_1)-k_2)f^\prime(k_1).
\]
Or in a more compact way the \textbf{envelope condition} here is:
\[v^\prime (k_{t})=U^\prime(f(k_{t})-k_{t+1})f^\prime(k_{t}).
\]
We can use this to get rid of term in FOC:
\[U^\prime(f(k_0)-k_1)=\beta U^\prime(f(k_1)-k_2)f^\prime(k_1).
\]

\end{frame}
\begin{frame}{Special Case}
    Assume $f(k_t)=k_t^\alpha$ and $u(c_t)=\ln c_t$. Let's solve by a Lagrangian instead of substituting the constraint into the objective function.
    The problem is stated:
    \[v(k_t)=\max_{c_t, k_{t+1}} \left[ \ln c_t+\beta v(k_{t+1})\right]
    \]
    such that 
    \[c_t+k_{t+1}=k_t^\alpha.
    \]
    In Bellman Form:
    \[v(k_t)=\max_{c_t, k_{t+1}} \left[ \ln c_t+\beta v(k_{t+1})\right]+\lambda_t(k_t^\alpha-c_t-k_{t+1})
    \]

\end{frame}
\begin{frame}{Special Case}
 \[v(k_t)=\max_{c_t, k_{t+1}} \left[ \ln c_t+\beta v(k_{t+1})\right]+\lambda_t(k_t^\alpha-c_t-k_{t+1})
    \]
    Differentiate to derive the first order conditions:
    \[\frac{1}{c_t}-\lambda_t=0,
    \]
    \[\beta v^\prime (k_{t+1})-\lambda_t=0,
    \]
    or combining them:
     \[\frac{1}{c_t}=\beta v^\prime (k_{t+1}).
    \]
    Using Envelope theorem:
    \[v^\prime (k_{t+1})=U_{t+1}^\prime f^\prime (k_{t+1})=\alpha \frac{1}{c_{t+1}}k_{t+1}^{\alpha-1} 
    \]
    \end{frame}
\begin{frame}{Special Case}
 \[\frac{1}{c_t}=\beta v^\prime (k_{t+1}).
    \]
     \[v^\prime (k_{t+1})=U_{t+1}^\prime f^\prime (k_{t+1})=\alpha \frac{1}{c_{t+1}}k_{t+1}^{\alpha-1} .
    \]
    Substitute envelope condition into FOC:
     \[\frac{1}{c_t}=\beta \alpha \frac{1}{c_{t+1}}k_{t+1}^{\alpha-1} .
    \]
    So the necessary equations for this problem are the equation above and the budget constraint:
    \[c_t+k_{t+1}=k_t^\alpha.
    \]
\end{frame}
\begin{frame}{Solution by Iterative Substitution}
    In this special case we can solve explicitly for a solution. Rewrite the FOC and budget constraint:
    \[\frac{k_{t+1}}{c_t}=\alpha \beta \frac{k_{t+1}^\alpha}{c_{t+1}}.
    \]
    \[\frac{k_{t}^\alpha}{c_t}-1=\frac{k_{t+1}}{c_t}.
    \]
    Substitute FOC into the constraint:
  \[\alpha \beta \frac{k_{t+1}^\alpha}{c_{t+1}}=
    \frac{k_{t}^\alpha}{c_t}-1\Rightarrow \frac{k_{t}^\alpha}{c_t}=1+\alpha \beta \frac{k_{t+1}^\alpha}{c_{t+1}}
    \]
    Update one period the consolidated condition above and substitute it into itself:
    \[\frac{k_{t}^\alpha}{c_t}=1+\alpha \beta (1+\alpha \beta \frac{k_{t+2}^\alpha}{c_{t+2}})
    \]
    \end{frame}
\begin{frame}{Solution by Iterative Substitution}
 \[\frac{k_{t}^\alpha}{c_t}=1+\alpha \beta (1+\alpha \beta \frac{k_{t+2}^\alpha}{c_{t+2}})
    \]
Do this recursively. Note that it is a geometric progression:
 \[\frac{k_{t}^\alpha}{c_t}=1+\alpha \beta +(\alpha \beta)^2+(\alpha \beta)^3+... =\frac{1}{1-\alpha \beta }.
    \]
    So the policy function is:
    \[c_t=(1-\alpha \beta )k_t^\alpha.
    \]
    Note that this is the answer we guessed earlier, based on the finite horizon problem. 
\end{frame}
\begin{frame}{Other Solution Methods: Solution by Conjecture}
   Suppose we suspect because of the form of the utility function that the amount that the household saves should be a constant fraction of their income, but we don't know what this fraction is:
   \[k_{t+1}=\theta k_t^\alpha.
   \]
   or equivalently
   \[c_t=(1-\theta)k_t^\alpha.
   \]
   Divide the two equations above:
   \[\frac{k_{t+1}}{c_t}=\frac{\theta}{1-\theta}
   \]
   and substitute into FOC:
   \[\frac{k_{t+1}}{c_t}=\alpha \beta \frac{k_{t+1}^\alpha}{c_{t+1}}
   \]
   \[\alpha \beta \frac{k_{t+1}^\alpha}{c_{t+1}}=\frac{\theta}{1-\theta}
   \]\end{frame}
\begin{frame}{Solution by Conjecture}
   Substituting the consumption function for $c_{t+1}$:
    \[\alpha \beta \frac{k_{t+1}^\alpha}{(1-\theta)k_{t+1}^\alpha}=\frac{\theta}{1-\theta}
   \]
   so 
   \[\alpha \beta =\theta.
   \]
   We reach the same solution as before:
   \[c_t=(1-\alpha)k_t^\alpha.
   \]
\end{frame}
\section{ Differential Equations}
\begin{frame}{Ordinary Differential Equations}
      \begin{block}{Definition}
    An \textbf{ordinary differential equation} is an equation $y'=F(y,t)$ between the derivative of an unknown function $y(t)$ and an expression $F(y,t) $ involving $y$ and $t$. 
    If the equation can be written as $y'=F(y)$, we call it an \textbf{autonomous} or \textbf{time independent} differential equation. For example, $y'=ay$ is time independent, and $y'=t^2$ is \textbf{nonautonomous} or \textbf{time-dependent.} \\
    
    An equation that involves derivatives up to and including the $i$th derivative is called an $i$-\textbf{th order} differential equation. 
    \end{block}
\end{frame}


\begin{frame}{Linear First Order Equations}
    \begin{block}
    {Examples}
    \begin{enumerate}
        \item $y'=ay,$ where $a$ is a constant.
        \item $y'=ay+b$.
        \item $y'=a(t)y$. 
        \item $y'=a(t)y+b(t)$.
        \item The general separable equation: $y'=a(y)h(t)$.
        \item $y'=y^2$. 
        \item $y'=y(a-by)$.
    \end{enumerate}
    \end{block}
\end{frame}


\begin{frame}{Linear Second Order Equations}

Suppose we are looking at:
\begin{equation}
    ay^{\prime \prime}+by'+cy=0.
    \label{diff2}
\end{equation}

For this differential equation, the polynomial 
\[ar^2+br+c=0,
\] is called the \textbf{characteristic equation}. 
\begin{block}{Theorem}
    If the characteristic polynomial of the linear second order differential equation \eqref{diff2} has distinct real roots $r_1$ and $r_2$, then the general solution of \eqref{diff2} is 
    \[y(t)=k_1e^{r_1t}+k_2e^{r_2t}.
    \]
    If $r_1=r_2$, then the general solution is 
    \[y(t)=k_1e^{r_1t}+k_2te^{r_1t}.
    \]
    \end{block}
\end{frame}


\begin{frame}{Nonhomogeneous Second Order Equations}  
Suppose we are looking at:
\begin{equation}
    ay^{\prime \prime}+by'+cy=g(t).
    \label{diff2nh}
\end{equation}
The function $g(t)$ is often called a \textbf{forcing term}.
    \begin{block}{Theorem}
    Let $y_p(t)$ be any particular solution of the homogeneous differential equation \eqref{diff2nh}. Let $k_1y_1(t)+k_2y_2(t)$ be a general solution of the corresponding homogeneous equation $ay"+by'+cy=0$. Then, the general solution of \eqref{diff2nh} is 
    \[y(t)=k_1y_1(t)+k_2y_2(t)+y_p(t).
    \]
    \end{block}
\end{frame}


\begin{frame}{Method of Undetermined Coefficients}
    \begin{itemize}
        \item The simplest method for finding a particular solution of \eqref{diff2nh} is called the \textbf{method of undetermined coefficients.}
        \item In this method, we look for a particular solution of equation \eqref{diff2nh} which has the same form as the forcing term $g(t).$
        \item If $g(t)$ is a polynomial in $t$ of order $j$, one looks for a particular solution which is also a polynomial in $t$ of order $j$. 
        \item If $g(t)$ is exponential, one looks for a particular solution that is a multiple of it.
    \end{itemize}
\end{frame}
\begin{frame}{Fundamental Theorem of Differential Equations}
  \begin{block}{Theorem}
    Consider the initial value problem
    \begin{align}
        \begin{array}{cc}
             y'=f(t,y), & y(t_0)=y_0.
        \end{array}
        \label{init}
    \end{align}
    Suppose that $f$ is continuous function at the point $(t_0, y_0)$. Then, there exists a $C^1$ function $y:I\to \mathbb{R}$ defined on an open interval $I=(t_0-a, t_0+a)$ about $t_0$ such that $y(t_0)=y_0$ and $y'(t)=f(t,y(t)) $ for all $t\in I$, that is, $y(t)$ is a solution of the initial value problem \eqref{init}. Furthermore, if $f$ is $C^1$ at $(t_0, y_0)$, then the solution $y(t)$ is unique.
    \\ This theorem holds for $j$th order differential equations provided that we specify $j$ initial conditions.
    \[y(t_0)=y_0, \text{ }y'(t_0)=y_1, \cdots, y^{[j-1]}(t_0)=y_{j-1}.
    \]
    \end{block}  
\end{frame}

\begin{frame}{Linear Systems of Differential Equations}
    The general linear system of equations can be written as:
    \begin{align*}
        y_1'=a_{11}y_1+\cdots +a_{1n}x_n\\
        y_2'=a_{21}y_1+\cdots +a_{2n}y_n\\ 
        \vdots \\
        y_n'=a_{n1}y_1+\cdots +a_{nn}x_n\\
    \end{align*}
    Which can be written as:
    \begin{equation}
        y'=Ay.\label{diffmatrix}
    \end{equation}
    \end{frame}

\begin{frame}{Linear Systems of Differential Equations}
\begin{block}{Theorem}
   Suppose that the $n\times n$ matrix $A$ has $n$ distinct real eigenvalues $r_1,\cdots, r_n$, with corresponding eigenvectors $v_1,\cdots, v_n$. Then, the general solution of the linear system  $y'=Ay$ of differential equations is
   \[y(t)=c_1e^{r_1t}v_1+c_2e^{r_2t}v_2+\cdots+c_ne^{r_nt}v_n.
   \]
   
    \end{block}
\begin{itemize}
    \item An alternative approach to solving \eqref{diffmatrix} is by substitution. 
\end{itemize}
\end{frame}


\section{Difference Equations}
\begin{frame}{Time Series}
    \begin{itemize}
        \item A set of data ordered by time forms a time series, $\{y_t\}_{t=1}^\infty$. 
        \item The properties \textit{frequency, time span, mean, variance, covariance} are used to give a basic description of time series. 
        \item White noise is a term frequently used in time series econometrics. White noise is a time series is a time series that does not contain any information that would help in estimation.
        \item For example,  a series of identical and independently distributed random variables with 0 mean is white noise. 
    \end{itemize}
\end{frame}

\begin{frame}{Difference Equations}
\begin{itemize}
    \item Time series data generating processes are in mathematical terminology called \textit{difference equations}. \item The theory of difference equations constitutes the basic mathematical background for time series econometrics.
    \item Formally, a \textbf{$p$th order linear difference equation} can be written as 
    \[y_t=a_0+\sum_{i=1}^pa_iy_{t-i}+x_t,
    \]
    where $x_t$ is the so-called \textit{forcing process} that can be any function of time $t$, current and lagged values of variables other than $y$.
    \item If $x_t$ is a  function of stochastic variables, then our system is called \textbf{stochastic difference equation.} 
\end{itemize}
\end{frame}

\begin{frame}{Difference Equations}
\begin{itemize}
    \item \textbf{ARMA} (autoregressive moving average) models are the most common processes used to estimate time series. 
    \item \textit{Autoregressive process of the $p$th order, AR($p$)}, is described as
    \[y_t=a_0+\sum_{i=1}^pa_iy_{t-i}+\varepsilon_t.
    \]
      \item \textit{Moving average of the $q$th order, MA($q$)}, is described as
    \[y_t=\sum_{i=0}^q\beta_i\varepsilon_{t-i}.
    \]
      \item \textit{Autoregressive moving average process of the $p,q$th order, ARMA($p,q$)}, is described as
    \[y_t=a_0+\sum_{i=1}^pa_iy_{t-i}+\sum_{i=0}^q\beta_i\varepsilon_{t-i}.
    \]
\end{itemize}

\end{frame}
\begin{frame}{Lag Operator}
    \begin{itemize}
        \item The \textbf{lag operator $L$} is a linear operator, which when applied to a variable $y_t$ yields its lagged value $y_{t-1}$.  
        \[Ly_t=y_{t-1}, \text{ or for higher lags }L^iy_t=y_{t-i}.
        \]
        \item Equations describing \textit{ARMA(p,q)} process, can be written with the use of lag operator as
        \[A(L)y_t=a_0+B(L)\varepsilon_t,
        \] where $A(L)$ and $B(L)$ are the following polynomials of $L:$
        \[A(L)=1-a_1L-a_2L^2-...-a_pL^p,
        \] and 
        \[B(L)=\beta_0+\beta_1L+\beta_2L^2+...+\beta_qL^q.
        \]
    \end{itemize}
    
    
\end{frame}
\begin{frame}{Lag Operator}
Although $L$ is an operator, most of its algebraic properties are analogous to those of a simple variable. The major properties of the lag operator $L$ are listed below:
\begin{enumerate}
    \item $L$ applied to a constant yields a constant: $Lc=c$.
    \item The distributive law holds: $(L^i+L^j)y_t=L^iy_t+L^jy_t=y_{t-i}+y_{t-j}$.
    \item The associative law for multiplication holds: $(L^iL^j)y_t=L^{i+j}y_t=y_{t-i-j}$.
    \item $L$ raised to a negative power is a \textit{ lead operator}: $L^{-i}y_t=y_{t+i}$. 
    \item From 2 and 3 above follows that for $|a|<1$, $(1+aL+a^2L^2+a^3L^3+...)y_t=\frac{y_t}{1-aL}$.
\end{enumerate}
\end{frame}

\begin{frame}{The Solution of Difference Equations}
   \begin{itemize}
       \item Solving a general linear difference equation involves expressing the value of $y_t$ as a function of the elements of the forcing process sequence $\{x_t \}$, time $t$, and potentially also of the elements of the sequence $\{y_t\}$ called \textit{initial conditions}.
       \item The solution process is quite similar to the process of solving linear differential equations. 
       \item In the general case, finding the solution involves finding homogeneous solution, particular solution and the expressing general solution as a linear combination of the other two. 
   \end{itemize} 
  \end{frame}

\begin{frame}{The Solution of Difference Equations}
\begin{itemize}
    \item The homogeneous part of the linear difference equation is defined as
    \[y_t=\sum_{i=1}^pa_iy_{t-i}, \text{ or, }y_t-\sum_{i=1}^pa_iy_{t-i}=0,
    \]
    which means that the constant $a_0$ and the forcing process $x_t$ from the original equation are left out. 
    \item Homogeneous equation has $p$ linearly independent solutions. 
    \item Homogeneous solutions are functions of time $t$ only. 
    \item Any linear combination of the homogeneous solutions is also a solution to the system of difference equations.
\end{itemize}
  \end{frame}

\begin{frame}{The Solution of Difference Equations}
\begin{itemize}
    \item One solution of the whole linear difference equation must be found. Such a solution is called a \textbf{particular solution}. 
    \item A particular solution can be a function of time $t$ and the elements of the forcing process $\{x_t\}$.
    \item The \textbf{general solution } of a linear difference equation is any linear combination of the homogeneous solution plus the particular solution. Clearly there are infinitely many general solutions as there are infinitely many constants. \item If the \textbf{initial conditions} for the values of $y$ at the initial periods are specified, then the arbitrary constants can be eliminated by imposing these conditions on the general solutions. 
    \item If the initial values are known for $p$ initial time periods $(y_0,y_1,..., y_{p-1})$, then all arbitrary constants can be eliminated and we obtain one \textbf{unique solution}.
\end{itemize}
\end{frame}
\begin{frame}{One-Dimensional Equations}
   \begin{itemize}\item  Suppose that our equation is 
    \[y_{t+1}=ay_t, 
    \]
    for some constant $a$. 
    \item A solution of this difference equation is an expression for $y_t$ in terms of initial amount $y_0$, $a$ and $t$. 
    \item By assumption, 
    \[y_1=ay_0.
    \]
    Therefore, 
    \[y_2=ay_1=a^2y_0,
    \]
    \[y_3=ay_2=a^2y_1=a^3y_0
    \]
    and so on. The solution is:
    
    \[y_t=a^ty_0.
    \]
    \end{itemize}
\end{frame}
\begin{frame}{AR(1) Process}
\begin{itemize}
    \item The $AR(1)$ process $y_t=a_0+a_1y_t+\varepsilon_t $, $|a_1|<1$, can be written as 
    \[ A(L)y_t=a_0+\varepsilon_t, \text{ where } A(L)=(1-a_1L). 
    \]
    \item The solution is 
    \[y_t=\frac{a_0}{(1-a_1L)}+\frac{\varepsilon_t}{1-a_1L}. 
    \]
    \item Using the properties of the lag operator, the solution can be written as
    \[y_t=(1+a_1L+a_1^2L^2+...)a_0+(1+a_1L+a_1^2L^2+...)\varepsilon_t,
    \]
    which can be simplified as 
    \[y_t=\frac{a_0}{1-a_1}+\sum_{i=0}^\infty a_1^i\varepsilon_{t-i}.
    \]
\end{itemize}
    
\end{frame}

\begin{frame}{System of Difference Equations}
\begin{itemize}
    \item Suppose we are dealing with a system of difference equations
    \[ z_{t+1}=Az_{t}, 
    \]
    where $A$ is a $k\times k$ matrix. 
    \item Let $r_1, r_2,..., r_k$ be eigenvalues of $A$, and $v_1,v_2,..., v_k$ the corresponding eigenvectors. Form the matrix
    \begin{align*}
        P=\begin{pmatrix}v_1 & v_2&\cdots & v_k
        \end{pmatrix}
    \end{align*}
    \item If $P$ is invertible, then 
    \begin{align*}
        A^n=P\begin{pmatrix}r_1^n & \cdots & 0\\
        \vdots & \ddots & \vdots \\
        0 &\cdots & r_k^n
        \end{pmatrix}P^{-1}.
    \end{align*}
    \end{itemize}
    \end{frame}

\begin{frame}{System of Difference Equations}
\begin{block}{Theorem}
      Suppose we are dealing with a system of difference equations
    \[ z_{t+1}=Az_{t}, 
    \]
    with initial vector $z_0. $ The solution of this system is 
     \begin{align*}
        A^t=P\begin{pmatrix}r_1^t & \cdots & 0\\
        \vdots & \ddots & \vdots \\
        0 &\cdots & r_k^t
        \end{pmatrix}P^{-1}z_0.
    \end{align*}
    And the general solution of that system is 
    \[z_t=c_1r_1^tv_1+c_2r_2^tv_2+\cdots +c_kr_k^tv_k.
    \]
\end{block}
    
\end{frame}

\begin{frame}{Summary}
\begin{itemize}
    \item This summarizes our lecture series. 
    \item We have covered topics in matrix algebra, logic, set theory, probability theory, calculus, function theory, optimization, differential equations and difference equations. \item Suggested Further Reading: Simone $\&$ Blume, Chapters 17, 18, 19. 
    \item Thank you for participation and best of luck in your PhD!
    
\end{itemize}
    
\end{frame}







\end{document}
