\documentclass{beamer}

%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
\usetheme{Boadilla}
%\usetheme{boxes}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{default}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}
\usepackage{epsfig}

\usepackage{amssymb, graphicx, amsmath, amsthm}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\setbeamercovered{highly dynamic}
\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}

\title{\textsc{Mathematics for Economics PhD}}


\subtitle{Lecture 3} 
\author{Instructor:  Mariam Arzumanyan}


\date[UIUC, Fall 2021]{University of Illinois at Urbana-Champaign \\August 18, 2021 }


\subject{Lecture Session}

\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}
\pgfdeclareimage[height=0.7cm]{Illinilogo}{Illinilogo.png}
\logo{\pgfuseimage{Illinilogo}} 
% Let's get started
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\setbeamercovered{invisible}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Class Information}
\begin{itemize}
  
    \item \textbf{Phone:} **
    \item \textbf{Email:} mariama2@illinois.edu
\item \textbf{Office Hours:}  1:30 - 2:30 pm  by appointment
\item \textbf{Zoom: } ** 
\item \textbf{During Class:} Please have your Video on and Sound muted. 

\end{itemize}

\end{frame}
\begin{frame}{References}
	\begin{itemize}

\item	\textbf{Simon, Blume, Mathematics for Economists \textit{W.W.Norton, 1994}}
\item	Mas-Colell, Whinston, Green, Microeconomic Theory: Mathematical Appendix \textit{Oxford University Press,  1995 }
\item		Sydsater, Hammond, Seierstad, and Strom, (2008). Further mathematics for economic analysis. Pearson education. Financial Times/Prentice Hall, second edition.
\item		Stokey, Lucas, Prescott, (1989). Recursive Methods in Economics Dynamic, Harvard University Press.

 \end{itemize}   
\end{frame}
\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\section{Homogeneous Functions}
\begin{frame}{Homogeneous Functions}
\begin{block}
    {Definition}For any scalar $k$, a real-valued function $f(x_1,..., x_n)$ is \textbf{homogeneous of degree k} if 
\[f(tx_1,...,tx_n)=t^kf(x_1,..., x_n) \text{ for all }x_1,..., x_n \text{ and all }t>0.
\]
In any case, the domain of a homogeneous function must be a \textbf{cone}, a set with the property that whenever $x$ is in the set, every positive scalar multiple $tx$ of $x$ is in the set. 
\end{block}
    \begin{itemize}
        \item If a production function of a firm is homogeneous of degree one, we say that the firm exhibits \textbf{constant returns to scale.}
        \item If a production function of a firm is homogeneous of degree $k>1$, we say that the firm exhibits \textbf{increasing returns to scale.}
        If a production function of a firm is homogeneous of degree $k<1$, we say that the firm exhibits \textbf{decreasing returns to scale.}
    \end{itemize}
    
    
    
\end{frame}

\begin{frame}{Homogeneous Functions}

\begin{block}{Theorem }
 Let $f(x)$ be continuously differentiable function on an open cone in $\mathbb{R}^n$. If $f$ is homogeneous of degree $k$, its first order partial derivatives are homogeneous of degree $k-1$.

\end{block}
\begin{block}{Theorem }
Let $f(x)$ be continuously differentiable homogeneous function on the positive orthant. The tangent planes to the level sets $f$ have constant slope along each ray from the origin.
\end{block}
\end{frame}

\begin{frame}{Homogeneous Functions}
\includegraphics[scale=0.5]{Homog.PNG}
\end{frame}


\begin{frame}{Euler's Theorem}
    \begin{block}
{Theorem} 
Let $f(x)$ be continuously differentiable homogeneous function of degree $k$ on $\mathbb{R}_+^n. $ Then, for all $x$,
\begin{equation}
    x_1\frac{\partial f}{\partial x_1}(x)+x_2\frac{\partial f}{\partial x_2}(x)+\cdots +x_n\frac{\partial f}{ \partial x_n}(x)=kf(x), \label{euler}
\end{equation}
or, in gradient notation, 
\[x\cdot \nabla f(x)=k f(x).
\]
The opposite is also true. If Equation ~\eqref{euler} is true for all $x\in \mathbb{R}_+^n$, then $f$ is homogeneous of degree $k$. 
    \end{block}
\end{frame}

\begin{frame}{Homogenizing a Function}
    \begin{block}{Theorem}
    Let $f(x_1,...,x_n)$ be a real-valued function defined on a cone $C$ in $\mathbb{R}^n$. Let $k$ be an integer. Define a new function $F$ of $n+1$ variables by 
    \[F(x_1,x_2,..., x_n,z)=z^kf\left(\frac{x_1}{z}, \cdots,\frac{x_n}{z}\right).
    \]
    Then, F is a homogeneous function of degree $k$ on the cone $C\times \mathbb{R}_+$ in $\mathbb{R}_{n+1}$. Since $f(x)=F(x,1)$ for all $x\in C$, we can consider $f$ as the restriction of $F$ to an $n$-dimensional subset of $\mathbb{R}_{n+1}$. 
    \end{block}
\end{frame}

\begin{frame}{Cardinal Versus Ordinal}
    \begin{block}{Definition}
    Let $I$ be an interval on the real line. Then, $g:I\to \mathbb{R}$ is a \textbf{monotonic transformation} of $I$ if $g$ is a strictly increasing function on $I$. Furthermore, if $g$ is a monotonic transformation and $u$ is a real-valued function of $n$ variables, then 
    \[g \circ u:x\to g(u(x))
    \]
    is a \textbf{monotonic transformation of u}.
    \end{block}
    \begin{block}{Definition}
    A characteristic of functions is called \textbf{ordinal} if every monotonic transformation of a function with this characteristic still has this characteristic. 
    \textbf{Cardinal} properties are not preserved by monotonic transformations. 
    \end{block}
\end{frame}

\begin{frame}{Homothetic Functions}
    \begin{block}{Definition}
    A function $v:\mathbb{R}_+^n\to \mathbb{R}$ is called \textbf{homothetic} if it is a monotone transformation of a homogeneous function, that is, if there is a monotonic transformation $g(z)$ of $\mathbb{R}_+$ and a homogeneous function $u:\mathbb{R}_+^n\to \mathbb{R}_+$ such that $v(x)=g(u(x))$ for all x in the domain. 
    \end{block}
    \begin{itemize}
        \item By definition, homotheticity is an ordinal property. 
        \item A monotonic transformation of a homothetic function is still homothetic.
    \end{itemize}
    \end{frame}

\begin{frame}{Homothetic Functions}
\begin{block}{Theorem}
Let $u: \mathbb{R}_+^n\to \mathbb{R}$ be a strictly monotonic function. Then, $u$ is homothetic if and only if for all $x$ and $y$ in $\mathbb{R}_+^n$, 
\[u(x)\geq u(y) \iff u(\alpha x)\geq u(\alpha y) \text{ for all }\alpha>0.
\]
\end{block}
\begin{block}{Theorem}
Let $u$ be a continuously differentiable function on $\mathbb{R}_+^n$. If $u$ is homothetic, then the slopes of tangent planes of the level sets of $u$ are constant along rays from the origin:

\[\frac{\frac{\partial u}{\partial x_i}(tx)}{\frac{\partial u}{\partial x_j}(tx)}=\frac{\frac{\partial u}{\partial x_i}(x)}{\frac{\partial u}{\partial x_j}(x)} \text{ for all }t>0.
\]
\end{block}
If $u$ is homothetic, then its marginal rate of substitution is a homogeneous function of degree zero.
\end{frame}
\section{Concave and Convex Functions}
\begin{frame}{Concave and Convex Functions}
    \begin{block}{Definition}
    A real-valued function $f$ defined on a convex subset $U$ of $\mathbb{R}^n$ is \textbf{concave} if for all $x, y$ in $U$ and for all $t$ between 0 and 1, 
    \[f(tx+(1-t)y)\geq tf(x)+(1-t)f(y).
    \]
      A real-valued function $f$ defined on a convex subset $U$ of $\mathbb{R}^n$ is \textbf{convex} if for all $x, y$ in $U$ and for all $t$ between 0 and 1, 
    \[f(tx+(1-t)y)\leq tf(x)+(1-t)f(y).
    \]
    \end{block}
    
    \begin{itemize}
        \item Notice that  $f$ is concave if and only if $-f$ is convex. 
        \item To every property of concave functions, there is corresponding property of convex functions. 
        
    \end{itemize}
    \end{frame}

\begin{frame}{Concave and Convex Functions}

\begin{block}{Theorem}
Let $f$ be a function defined on a convex subset $U$ of $\mathbb{R}^n$. Then, $f$ is concave (convex) if its restriction to every line segment in $U$ is a concave (convex) function of one variable. 

\end{block}
For functions of one variable, we know that:
\begin{itemize}
\item Concave functions are continuous. 
    \item A $C^1$ function on an interval $I$ is concave if and only if its first derivative $f'(x)$ is a decreasing function of $x$ on $I$. 
    \item A $C^2$ function $f$ is concave on an interval $I$ if and only if its second derivative $f''(x)\leq 0$ for all $x$ in $I$.
\end{itemize}
   \end{frame}

\begin{frame}{Concave and Convex Functions}

\begin{block}{Theorem}
Let $f$ be a $C^1$ function defined on a convex subset $U$ of $\mathbb{R}^n$. Then, $f$ is concave on $U$ if and only if for all $x, y\in U$:
\[f(y)-f(x)\leq Df(x)(y-x);
\]
that is;
\[f(y)-f(x)\leq \frac{\partial f }{\partial x_1}(x) (y_1-x_1)+...+\frac{\partial f}{\partial x_n}(x)(y_n-x_n).
\]
Similarly,  $f$ is convex on $U$ if and only if $f(y)-f(x)\geq Df(x) (y-x)$ for all $x, y\in U$.
\end{block}
  \end{frame}

\begin{frame}{Concave and Convex Functions}

\begin{block}{Theorem}
If $f$ is a $C^1$ concave function on a convex set $U$ and if $x_0\in U$, then 
\[Df(x_0)(y-x_0)\leq 0 \text{ implies }f(y)\leq f(x_0).
\]
In particular, if $Df(x_0)(y-x_0)\leq 0$ for all $y\in U$, then $x_0$ is a global max of $f$.
\end{block}
\begin{block}{Theorem}
Let $f$ be a $C^2$ function on an open convex subset of $\mathbb{R}^n$. Then, $f$ is a concave function on $U$ if and only if the Hessian $D^2f(x)$ is negative semidefinite for all $x\in U$. The function $f$ is a convex function on $U$ if and only if $D^2f(x)$ is positive semidefinite. 
\end{block}
\end{frame}
\begin{frame}{Properties of Concave Functions}
    \begin{block}{Theorem}
    Let $f$ be a concave (convex) function on an open, convex subset $U$ of $\mathbb{R}^n$. If $x_0$ is a critical point of $f$, that is, $Df(x_0)=0$, then $x_0\in U$ is a global maximizer (minimizer) of $f$ on $U$.
    \end{block}
    \begin{block}{Theorem}
    Let $f_1,\cdots, f_k$ be concave (convex) functions, each defined on the same convex subset $U$ of $\mathbb{R}^n$. Let $a_1,\cdots, a_k$ be positive numbers. Then, $af_1+\cdots a_kf_k$ is a concave (convex) function on $U$.
    \end{block}
    \end{frame}
\begin{frame}{Properties of Concave Functions}

\begin{block}{Theorem}
Let $f$ be a  function defined on an open, convex subset $U$ of $\mathbb{R}^n$. If $f$ is concave, then for every $x_0$ in $U$, the set
\[C_{x_0}^+=\{x\in U:f(x)\geq f(x_0)\}
\]
is a convex set. If $f$ is convex, then for every $x_0$ in $U$, the set
\[C_{x_0}^-=\{x\in U:f(x)\leq f(x_0)\}
\] is a convex set. 
\end{block}
\end{frame}

\begin{frame}{Quasiconcave and Quasiconvex Functions}
    \begin{block}{Definition}
    A function $f$ defined on a convex subset  $U$ of $\mathbb{R}^n$ is \textbf{quasiconcave} if for every real number $a$, 
    \[C_{a}^+=\{x\in U:f(x)\geq a\}
\]
is a convex set. Similarly, $f$ is \textbf{quasiconvex} if for every real number $a$, \[C_{a}^-=\{x\in U:f(x)\leq a\}
\] is a convex set. 
    \end{block}
    \begin{itemize}
        \item Every Cobb-Douglas function $F(x, y)=Ax^ay^b$ is quasiconcave for $a, b$ positive. 
    \end{itemize}
    \end{frame}

\begin{frame}{Quasiconcave and Quasiconvex Functions}
\begin{block}{Theorem}
Let $f$ be a function defined on a convex set $U\subset \mathbb{R}^n$. Then, the following statements are equivalent:
\begin{enumerate}
    \item $f$ is a quasiconcave function on $U$. 
    \item For all $x,y\in U$ and all $t\in [0,1]$, 
    \[f(x)\geq y \text{ implies }f(tx+(1-t)y)\geq f(y).
    \]
    \item For all $x,y\in U$ and all $t\in [0,1]$,
    \[ f(tx+(1-t)y\geq \min \{f(x), f(y)\}.
    \]
\end{enumerate}
\end{block}
\begin{itemize}
    \item 
 All concave functions are quasiconcave, but not every quasiconcave function is concave. 
 \end{itemize}
\end{frame}
\begin{frame}{Properties of Quasiconcave Functions}
    \begin{itemize}
        \item An increasing function on $\mathbb{R}$ is both quasiconcave and quasiconvex.
        \item The bell-shaped functions (pdf of Normal-distribution) are quasiconcave. 
        \item If $f$ is homogeneous of degree one and quasiconcave, then it is concave. 
        \item $f$ is quasiconcave on $U\subset \mathbb{R}^n$ if and only if 
        \[f(y)\geq f(x) \text{ implies that }Df(x)(y-x)\geq 0.
        \]
        \item Every monotonic transformation of a quasiconcave function is quasiconcave function. It is an ordinal property. Concevity, on the other hand, is a cardinal property.
     \end{itemize}
\end{frame}
\begin{frame}{Properties of Quasiconcave Functions}
\begin{block}{Theorem}
Let $F$ be a $C^2$ function on a convex set $W$ in $\mathbb{R}^2$. Suppose that $F_x^\prime>0$, $F_y^\prime>0$ on W. If the determinant 
\begin{align*}
    det\begin{pmatrix} 0& F_x^\prime & F_y^\prime \\
     F_x^\prime & F_{xx}^{\prime \prime} & F_{xy}^{\prime \prime} \\
     F_y^\prime & F_{xy}^{\prime \prime} & F_{yy}^{\prime \prime} 
    \end{pmatrix}
\end{align*}
is $>0$ for all $(x,y)\in W$, then $F$ is quasiconcave on $W$. If the determinant is negative for all $(x,y)\in W$, then $F$ is quasiconvex on $W$.
\par Conversely, if $F$ is quasiconcave on $W$, then the determinant is $\geq 0$; if $F$ is quasiconvex on $W$, then the determinant is $\leq 0$ for all $(x,y)\in W$.
\end{block}

\end{frame}


\section{Correspondences}
\begin{frame}{Correspondences}
\begin{block}{Definition}
    Given a set $A\subset \mathbb{R}^n$, a \textbf{correspondence }$f:A\to \mathbb{R}^k$ is a rule that assigns a set $f(x)\subset \mathbb{R}^k$ for every $x\in A$. 
\end{block}
    \begin{itemize}
        \item Note that when, for every $x\in A$ $f(x)$ is composed of precisely one element, then $f(\cdot)$ can be viewed as a function. 
        \item The definition allows $f(x)=\emptyset$, but typically we only consider correspondences with $f(x)\neq \emptyset$. 
        \begin{block}{Definition}
            Given $A\subset \mathbb{R}^n$ and $Y\subset \mathbb{R}^k$, the \textbf{graph} of the correspondence $f:A\to Y$ is the set
            \[\{(x,y)\in A\times Y: y\in f(x)\}.
            \]
        \end{block}
    \end{itemize}
\end{frame}
\begin{frame}{Correspondences}
\begin{block}{Definition}
Given a set $A\subset \mathbb{R}^n$ and  the closed set $Y\subset \mathbb{R}^k$, the correspondence $f:A\to Y$ has a \textbf{closed graph} if for any two sequences $x^m\to x \in A$ and $y^m\to y, $ with $x^m\in A$ and $y^m\in f(x^m)$ for every $m$, we have $y\in f(x)$. 
\end{block}
\begin{itemize}
    \item The concept of a closed graph is simply our usual notation of closedness applied to the set $\{(x,y)\in A\times Y: y\in f(x)\}$
\end{itemize}
\begin{block}{Definition}
Given a set $A\subset \mathbb{R}^n$ and  the closed set $Y\subset \mathbb{R}^k$, the correspondence $f:A\to Y$ has a \textbf{upper hemicontinuous (uhc)} if it has a closed graph and the images of compact sets are bounded, that is, for every compact set $B\subset A$ the set $f(B)=\{y\in Y: y\in f(x) \text{ for some } B\}$ is bounded. 
\end{block}
\end{frame}
\begin{frame}{Correspondences}
\begin{itemize}
    \item The upper hemicontinuity property for correspondences can be viewed as a generalization of continuity for functions. 
\end{itemize}
\begin{block}{Theorem}
Given $A\subset \mathbb{R}^n$ and the closed set $Y\subset \mathbb{R}^k$, suppose that $f:A\to Y$ is a single-valued correspondence (function). Then, $f(\cdot )$ is an upper hemicontinous correspondence if and only if it is continuous  as a function. 
\end{block}
\begin{itemize}
    \item If $f(\cdot )$ is continuous as a function, then $f(\cdot )$ has a closed graph, and the images of compact sets under $f$ are compact.
\end{itemize}
\end{frame}
\begin{frame}{Correspondences}
\begin{block}{Definition}
Given $A\subset \mathbb{R}^n$ and the closed set $Y\subset \mathbb{R}^k$, the correspondence $f:A\to Y$ is \textbf{lower hemicontinuous (ihc)} if for every sequence $x^m\to x\in A$ with $x^m\in A$ for all $m$, and every $y\in f(x)$, we can find a sequence $y^m\to y$ and an integer $M$ such that $y^m\in f(x^m)$ for $m>M$.
\end{block}\begin{itemize}
    \item If $f(\cdot ) $ is a function then the concepts of lower hemicontinuity as a correspondence and of continuity as a function coincide. \item When a correspondence is both upper and lower hemicontinuous, we say that it is \textbf{continuous}.
\end{itemize}
\end{frame}
\section{Fixed Point Theorems}
\begin{frame}{Fixed Point Theorems}
\begin{itemize}
    \item In economics the most frequent technique for establishing the existence of solutions to an equilibrium system of equations consists of setting up the problem as the search for \textit{fixed point} of a function or correspondence $f:A\to A$.
    \item A vector $x\in A$ is a \textbf{fixed point} of $f(\cdot )$ if $f(x)=x$ [or, in the correspondence case, if $x\in f(x)$].
    \item That is, the vector is mapped into itself and so it remains "fixed".
\end{itemize}
  \end{frame}

\begin{frame}{Fixed Point Theorems}
\begin{block}{Brouwer's Fixed Point Theorem}
Suppose $A\subset \mathbb{R}^N$ is a nonempty, compact, convex set, and $f:A\to A$ is a continuous function from $A$ into itself. Then $f(\cdot )$ has a fixed point; that is, there is an $x\in A$ such that $x=f(x)$.

\end{block}

\end{frame}

\begin{frame}{Fixed Point Theorems}
\begin{block}{Kakutani's Fixed Point Theorem}
Suppose that $A\subset \mathbb{R}^N$ is nonempty, compact, convex set, and that $f:A\to A$ is an upper hemicontinuous correspondence from A into itself with the property that the set $f(x) \subset A$ is nonempty and convex for every $x\in A$. Then $f(\cdot )$ has a fixed point; that is, there is an $x\in A$ such that $x\in f(x)$.

\end{block}
\end{frame}
\section{Integral Calculus}
\begin{frame}{Integral Calculus}
    \begin{block}{Definition}
    An \textbf{antiderivative} of a function $f(x)$ is a function $F(x)$ whose derivative is the original $F$: $F'=f$. The function $F$ is called the \textbf{indefinite integral} of $f$ and written $F(x)=\int f(x)dx.$
    \end{block}
    The usual laws of differentiation yield the following table of indefinite integrals, where $C$ denotes an arbitrary constant:
    \begin{align*}
        \begin{array}{lr}
            \int af(x)dx=a \int f(x)dx; & \int (f+g)dx=\int fdx +\int gdx.  \\
          \int x^n dx=\frac{x^{n+1}}{n+1}+C, \text{ if }n\neq -1;   & \int \frac{1}{x}dx=\ln x+C. \\
          \int e^xdx=e^x+C; & \int e^{f(x)}f'(x)dx=e^{f(x)}+C.\\
          \int \left(f(x)\right)^nf'(x)dx=\frac{1}{n+1}\left(f(x)\right)^{n+1}+C & \\
          \int \frac{1}{f(x)}f'(x)dx=\ln f(x)+C. &
        \end{array}
    \end{align*}
\end{frame}

\begin{frame}{Integration by Parts}
    Another convinient rule for for computing antiderivatives is the converse of the Product rule. The Product rule states that, for two differentiable functions $u(x)$ and $v(x)$, 
    \[(u\cdot v)'=u'\cdot v+u \cdot v'. 
    \]
    Taking antiderivatives of both sides, we find 
    \[u\cdot v=\int u'\cdot v+\int u \cdot v'. 
    \]
    which is usually written as 
    \[\int u(x) v'(x)dx=u(x)v(x)- \int u'(x)v(x) dx
    \] and is called \textbf{integration by parts}
\end{frame}

\begin{frame}{ Leibniz Integral Rule}
    \begin{block}{ }
    \begin{align*}
        \dfrac{d}{dx}\left( \int_{a(x)}^{b(x)} f(x,t)dt\right)= f(x, b(x))\dfrac{d}{dx}b(x)-\\- f(x, a(x))\dfrac{d}{dx}a(x)+ \int_{a(x)}^{b(x)} \dfrac{\partial }{\partial x}f(x,t)dt
    \end{align*}
    \end{block}
\end{frame}

\section{One Variable Calculus}

\begin{frame}{One Variable Calculus}
    \begin{itemize}
        \item One of the major uses of calculus in mathematical models is to find and characterize maxima and minima of functions. 
        \item A function $f$ has a \textbf{local maximum} at $x_0$ if $f(x)\leq f(x_0)$ for all $x$ in some open interval containing $x_0$. $f$ has \textbf{global maximum} at $x_0$ if $f(x)\leq f(x_0)$ for all $x$ in the domain of $f$. 
        \item A function $f$ has a \textbf{local minimum} at $x_0$ if $f(x)\geq f(x_0)$ for all $x$ in some open interval containing $x_0$. $f$ has \textbf{global minimum} at $x_0$ if $f(x)\geq f(x_0)$ for all $x$ in the domain of $f$. 
        \item A max or min of a function can occur at an endpoint of the domain $f$ or at the interior point of the domain.
        
    \end{itemize}
    \end{frame}

\begin{frame}{One Variable Calculus}
\begin{block}{Theorem: FOC }
If $x_0$ is an interior max or min of a $C^1$ function $f$, then $f'(x_0)=0$.

\end{block}
\begin{block}{Second Order Conditions}
If $x_0$ is a critical point of a twice differentiable function $f$, then:
\begin{enumerate}
    \item If $f'(x_0)=0$ and $f"(x_0)<0$, then $x_0$ is a max of $f$;
     \item If $f'(x_0)=0$ and $f"(x_0)>0$, then $x_0$ is a min of $f$; and
      \item If $f'(x_0)=0$ and $f"(x_0)=0$, then $x_0$ can be a max or a min, or neither.
\end{enumerate}
\end{block}
\end{frame}
\section{Unconstrained Optimization}
\begin{frame}{Unconstrained Optimization}
    The definitions for a maximum or minimum for a function of several variables are the same as the corresponding definitions for a function of one variable. 
    \begin{block}{Definitions}
    Let $F:U\to \mathbb{R}^1$ be a real-valued function of $n$ variables, whose domain $U$ is a subset of $\mathbb{R}^n$. 
    \begin{enumerate}
        \item A point $x^*\in U$ is a \textbf{max } of $F$ on $U$ if $F(x^*)\geq F(x)$ for all $x\in U$. 
       \item A point $x^*\in U$ is a \textbf{strict max } of $F$ on $U$ if $F(x^*)> F(x)$ for all $x\neq x^*\in U$. 
       \item A point $x^*\in U$ is a \textbf{local max } of $F$ there is a ball $B_r(x^*)$ about $x^*$ such that $F(x^*)\geq F(x)$ for all $x\in B_r(x^*)\cap U$.
  \item A point $x^*\in U$ is a \textbf{strict local max } of $F$ there is a ball $B_r(x^*)$ about $x^*$ such that $F(x^*)> F(x)$ for all $x\in B_r(x^*)\cap U$.
    \end{enumerate}
    \end{block}
\end{frame}


\begin{frame}{First Order Conditions}
\begin{block}{Theorem}
    Let $F:U\to \mathbb{R}^1$ be a continuously differentiable, real-valued function of $n$ variables,defined on $U\subset \mathbb{R}^n$. If $x^*$ is a local max or min of $F$ in $U$ and if $x^*$ is an interior point of $U$, then
    
    \[\frac{\partial F}{\partial x_i}(x^*)=0 \text{ for }i=1,2,\cdots,n. 
    \]
\end{block}
    
\end{frame}
\begin{frame}{Second Order or Sufficient Conditions}
 \begin{block}{Theorem}
    Let $F:U\to \mathbb{R}^1$ be a $C^2$ function whose domain is an open set $U$ in $\mathbb{R}^n$. Suppose that $x^*$ is a critical point of $F$.
    \begin{enumerate}
        \item If the Hessian $D^2F(x^*)$ is a negative definite symmetric matrix, then $x^*$ is a strict local max of $F$;
        \item If the Hessian $D^2F(x^*)$ is a positive definite symmetric matrix, then $x^*$ is a strict local min of $F$;
        \item If $D^2F(x^*)$ is indefinite, then $x^*$ is neither a local max nor a local min of $F$. 
    \end{enumerate}
\end{block}   
\textbf{Definition: } A critical point $x^*$ of $F$ for which the Hessian $D^2F(x^*)$ is indefinite is called a \textbf{saddle point} of $F$. 
\end{frame}
\begin{frame}{Necessary Conditions}
\begin{block}{Theorem}
   Let  $F:U\to \mathbb{R}^1$ be a $C^2$ function whose domain  $U$ is in $\mathbb{R}^n$. Suppose that $x^*$ is an interior point of $U$ and that $x^*$ is a local max of $F$. Then, $DF(x^*)=0$ and $D^2F(x^*)$ is negative semidefinite. 
\end{block}
\begin{block}{Theorem}
      Let  $F:U\to \mathbb{R}^1$ be a $C^2$ function whose domain  $U$ is in $\mathbb{R}^n$. Suppose that $x^*$ is an interior point of $U$. 
      \begin{enumerate}
          \item If $x^*$ is a local min of $F$, then $(\frac{\partial F}{\partial x_i})(x^*)=0$ for $i=1,..., n $ and all principal minors of the Hessian $D^2F(x^*)$ are $\geq 0$.
          \item If $x^*$ is a local max of $F$, then $(\frac{\partial F}{\partial x_i})(x^*)=0$ for $i=1,..., n $ and all principal minors of the Hessian $D^2F(x^*)$ of odd order are $\leq 0$, and all principal minors of the Hessian $D^2F(x^*)$ of odd order are $\geq 0$.
      \end{enumerate}
\end{block}
\end{frame}
\begin{frame}{Global Maxima of Concave Functions}
    \begin{block}{Theorem}
     Let  $F:U\to \mathbb{R}^1$ be a $C^2$ function whose domain is a convex open subset  $U$ of $\mathbb{R}^n$. 
     \begin{enumerate}
         \item The following three conditions are equivalent:
         \begin{enumerate}[i]
             \item $F$ is a concave function of $U$; and
             \item $F(y)-F(x)\leq DF(x)(y-x)$ for all $x, y\in U$; and 
             \item $D^2F(x)$ is negative semidefinite for all $x\in U$.
         \end{enumerate}
         \item If $F$ is a concave function on $U$ and $DF(x^*)=0$ for some $x^*\in U$, then $x^*$ is a global max of $F$ on $U$. 
         \item If $F$ is a convex function on $U$ and $DF(x^*)=0$ for some $x^*\in U$, then $x^*$ is a global min of $F$ on $U$.  
     \end{enumerate}
\end{block}
\end{frame}
\section{Constrained Optimization}
\begin{frame}{Constrained Optimization}
    \begin{block}{Definition}
    Suppose we are looking at the following problem:
    \[\max f(x_1,...,x_n)
    \]
     where $(x_1,..., x_n) \in \mathbb{R}^n$ must satisfy
     \[g_1(x_1,..., x_n)\leq b_1,..., g_k(x_1,..., x_n)\leq b_k,
     \]
     \[h_1(x_1,..., x_n)= c_1,..., h_m(x_1,..., x_n)= c_m.
     \]
     The function $f$ is called the \textbf{objective function}, while $g_1,..., g_k$ and $h_1,..., h_m$ are called \textbf{constraint functions.} The $g_j$'s define \textbf{inequality constraints}, and $h_i$'s define the \textbf{equality constraints}. $x_1\geq 0$, ..., $x_n\geq 0 $ are called \textbf{nonnegativity constraints. }
\end{block}
\end{frame}
\begin{frame}{Equality Constraints}

Consider the problem of maximizing a function $f$ of $n$ variables constrained by $m$ equalities:
\begin{equation}
    \max f(x_1,...,x_n) \label{equal}
    \end{equation}
    
     where $(x_1,..., x_n) \in \mathbb{R}^n$ must satisfy
     
     \[h_1(x_1,..., x_n)= c_1,..., h_m(x_1,..., x_n)= c_m.
     \]
     \begin{block}{Definition}
         We say that $(h_1,..., h_m)$ satisfy the \textbf{nondegenerate constraint qualification (NDCQ)} at $x^*$ if the rank of the Jacobian matrix $Dh(x^*)$ at $x^*$ is $m$.
     \end{block}
\end{frame}
\begin{frame}{Equality Constraints: First Order Conditions}
\begin{block}{Theorem}
      Let $f, h_1,..., h_m$ be $C^1$ functions of $n$ variables. Consider problem ~\eqref{equal}. Suppose that $x^*$ is a local max or min of $f$. Suppose that $x^*$ satisfies NDCQ conditions. Then, there exists $\mu_1^*,..., \mu_m^*$ such that $(x_1^*,..., x_n^*, \mu_1^*,..., \mu_m^*)$ is a critical point of the Lagrangian 
    \[L(x, \mu)=f(x)-\mu_1(h_1(x)-c_1)-\mu_2(h_2(x)-c_2)-...-\mu_m(h_m(x)-c_m).
    \]
    \begin{align*}
        \begin{array}{ccc}
           \frac{\partial L}{\partial x_1}(x^*, \mu^*)=0,  & \cdots, & \frac{\partial L}{\partial x_n}(x^*, \mu^*)=0, \\
           & & \\
            \frac{\partial L}{\partial \mu_1}(x^*, \mu^*)=0,  & \cdots, & \frac{\partial L}{\partial \mu_m}(x^*, \mu^*)=0.
        \end{array}
    \end{align*}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inequality Constraints}
\begin{frame}{One Inequality Constraint}
   \begin{block}{Theorem}
    Suppose that $f$ and $g$ are $C^1$ functions on $\mathbb{R}^2$ and $(x^*, y^*)$ maximizes $f$ on the constraint set $g(x,y)\leq b$. If $g(x^*, y^*)=b$, suppose that $\frac{\partial g}{\partial x}(x^*, y^*)\neq 0$ or $\frac{\partial g}{\partial y}(x^*, y^*)\neq 0$. Form the Lagrangian function
    \[L(x,y,\lambda)= f(x,y)-\lambda [g(x,y)-b]. 
    \]
    Then, there is a multiplier $\lambda^*$ such that:
    \begin{enumerate}
        \item $\frac{\partial L}{\partial x}(x^*, y^*, \lambda^*)=0$, 
        \item $\frac{\partial L}{\partial y}(x^*, y^*, \lambda^*)=0$, 
        \item $\lambda^*[g(x^*, y^*)-b]=0$.
    \end{enumerate}
\end{block} 
\end{frame}
\begin{frame}{Several Inequality Constraints}
   \begin{block}{Theorem}
    Suppose that $f, g_1,..., g_k$ are $C^1$ functions on $n$ variables. Suppose that $x^*\in \mathbb{R}^n$ is a local maximizer of $f$ on the constraint set defined by $k$ inequalities. 
    \[g_1(x_1,..., x_n)\leq b_1,..., g_k(x_1,..., x_n)\leq b_k.
    \]
    Assume that the first $k_0$ constraints are binding at $x^*$ and the last $k-k_0$ constraints are not binding. Suppose that the following nondegenerate constraint qualification is satisfied at $x^*$. 
    The rank at $x^*$ of the $Jacobian $ matrix of the binding constraint 
    \begin{align*}
        \begin{pmatrix}
            \frac{\partial g_1}{\partial x_1}(x^*) & \cdots & \frac{\partial g_1}{\partial x_n}(x^*)\\ 
            \vdots & \ddots &\vdots \\
            \frac{\partial g_{k_0}}{\partial x_1}(x^*) & \cdots & \frac{\partial g_{k_0}}{\partial x_n}(x^*)
        \end{pmatrix}
    \end{align*}
    is $k_0$- as large as it can be. 
\end{block} 
\end{frame}
\begin{frame}{Several Inequality Constraints}
   \begin{block}{Theorem}
   Form the Lagrangian
   \[L(x_1,x_2,..., x_n,\lambda_1,..., \lambda_k)= f(x)-\lambda_1 [g_1(x)-b_1]-\cdots -\lambda_k[g_k(x)-b_k]. 
    \]
    Then, there exist multipliers $\lambda_1^*,...,\lambda_k^*$ such that:
    \begin{enumerate}
        \item $\frac{\partial L}{\partial x_1}(x^*,  \lambda^*)=0,...,\frac{\partial L}{\partial x_n}(x^*,  \lambda^*)=0$, 
        \item $\lambda_1^*[g_1(x^*)-b_1]=0$, ..., $\lambda_k^*[g_k(x^*)-b_k]=0$,
        \item $\lambda_1^*\geq 0,..., \lambda_k^*\geq 0$,
        \item $g_1(x^*)\leq b_1,..., g_k(x^*)\leq b_k$.
    \end{enumerate}
   \end{block}
\end{frame}
\begin{frame}{Kuhn-Tucker Formulation}
    \begin{itemize}
        \item The most common constrained maximization problems in economics involve only inequality constraints and a complete set of nonnegativity constraints:
        \begin{align*}
        \begin{array}{c}
            \max f(x_1,..., x_n)\\
            \text{ subject to } g_1(x_1,..., x_n)\leq b_1,..., g_k(x_1,..., x_n)\leq b_k,\\ x_1\geq 0, x_2\geq 0,..., x_n\geq 0.
            \end{array}
        \end{align*}
    \item The Lagrangian of this problem:
    \[L(x,\lambda, \nu) =f(x)-\lambda_1[g_1(x)-b_1]-...-\lambda_k[g_k(x)-b_k]+\nu_1 x_1+...+\nu_nx_n.
    \]
    \end{itemize}
\end{frame}
\begin{frame}{The Meaning of The Multiplier}
    Suppose we are looking:
    \begin{align*}
        \begin{array}{c}
            \text{ maximize } f(x,y) \\
            \text{ subject to } h(x,y)=a.
        \end{array}
    \end{align*}
   \begin{block}{Theorem}
    Let $f$ and $h$ be $C^1$ functions of two variables. For any fixed value of the parameter $a$, let $(x^*(a), y^*(a))$ be the solution with corresponding multiplier $\mu^*(a)$. Suppose that $x^*, y^*, \mu^*$ are $C^1$ functions of $a$ and that $NDCQ$ holds at  $(x^*(a), y^*(a),\mu^*(a))$. Then,
    \[\mu^*(a)=\frac{d}{da}f(x^*(a), y^*(a)).
    \]
    $\mu^*(a)$ measures the rate of change of the optimal value of $f$ with respect to parameter $a$.
\end{block} 
\end{frame}
\begin{frame}{Inequality Constraints}
   \begin{block}{Theorem}
      Suppose $a^*=(a_1^*,..., a_k^*)$. Suppose that $x^*(a^*)\in \mathbb{R}^n$ is a solution of maximizer of $f$ on the constraint set defined by $k$ inequalities. 
    \[g_1(x_1,..., x_n)\leq a_1^*,..., g_k(x_1,..., x_n)\leq a_k^*.
    \]
    Let $\lambda_1^*(a^*),...,\lambda_k^*(a^*) $ be the corresponding Lagrange multipliers. Suppose that all functions are differentiable and NDCQ holds at $a^*$. Then, for each $j=1,..., k,$
    \[\lambda_j^*(a^*)=\frac{\partial }{\partial a_j}f(x_1^*(a^*),..., x_n^*(a^*)).
    \]
\end{block} 
\end{frame}

\begin{frame}{Envelope Theorem}
    \begin{block}{Unconstrained Problems: Theorem}
    Let $f(x, a)$ be a $C^1$ function of $x\in \mathbb{R}^n$ and the scalar $a$. For each choice of the parameter $a$, consider the unconstrained maximization problem
    \[\text{ maximize } f(x;a) \text{ with respect to }x.
    \]
        Let $x^*(a)$ be a solution of this problem. Suppose that $x^*(a)$ is a $C^1$ function of $a$. Then, 
        \[\frac{d}{da}f(x^*(a);a)=\frac{\partial}{\partial a }f(x^*(a); a).
        \]
        
\end{block} 
\end{frame}

\begin{frame}{Envelope Theorem}
    \begin{block}{Constrained Problems: Theorem}
    Let $f, h_1,...,h_k:\mathbb{R}^n\times \mathbb{R}\to \mathbb{R}^1$ be $C^1$ functions. Let $x^*(a)=(x_1^*(a),..., x_n^*(a))$ denote the solution of the problem of maximizing $f(x,a)$ on the constraint set 
    \[h_1(x,a)=0,..., h_k(x,a)=0,
    \]
    for any fixed choice of the parameter $a$. Suppose that $x^*(a)$ and the Lagrange multipliers $\mu_1(a),..., \mu_k(a)$ are $C^1$ functions of $a$ and that the NDCQ holds. Then, 
    \[\frac{d}{da}f(x^*(a), a)=\frac{\partial L}{\partial a}(x^*(a), \mu^*(a),a),
    \]
    where $L$ is the Lagrangian for this problem. 
    \end{block}
\end{frame}


\begin{frame}{Summary}
    
    \begin{itemize}
        \item Today, we discussed various desired properties of functions. 
        \item We discussed first order or necessary conditions for maximization problems. 
        \item Tomorrow, we will turn our attention to second order conditions, minimization problems, and concave/convex optimization. 
        \item Then, we will move to dynamic programming. 
    \end{itemize}
\end{frame}















\end{document}
